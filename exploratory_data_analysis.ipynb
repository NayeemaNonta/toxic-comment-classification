{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780197aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from process_text import clean_text, lemmatizing\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "train_data_pth = os.getenv(\"TRAIN_DATA_PATH\")\n",
    "test_data_pth = os.getenv(\"TEST_DATA_PATH\")\n",
    "\n",
    "# Define constants\n",
    "DPI = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838e93b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API authenticated successfully!\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "print(\"Kaggle API authenticated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09bf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_data_pth)\n",
    "test_df = pd.read_csv(test_data_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.head())\n",
    "train_df.info()\n",
    "train_df.describe(include='all')\n",
    "train_df.isnull().sum()\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_df[label_cols].sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44502136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label columns\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Calculate label counts and sort\n",
    "label_counts = train_df[label_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "# Total comments\n",
    "total_comments = len(train_df)\n",
    "\n",
    "# Number of non-toxic and toxic comments\n",
    "train_df['num_labels'] = train_df[label_cols].sum(axis=1)\n",
    "non_toxic_count = (train_df['num_labels'] == 0).sum()\n",
    "toxic_count = total_comments - non_toxic_count\n",
    "\n",
    "# Plot Toxic vs Non-Toxic\n",
    "plt.figure(figsize=(6, 4), dpi=DPI)\n",
    "plt.bar(['Non-Toxic', 'Toxic'], [non_toxic_count, toxic_count], color=['lightgreen', 'salmon'])\n",
    "plt.title(\"Toxic vs Non-Toxic Comments\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(10, 6), dpi=DPI)\n",
    "label_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Label Distribution in Toxic Comment Dataset\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display class imbalance stats as DataFrame\n",
    "label_stats = pd.DataFrame({\n",
    "    \"Label\": label_counts.index,\n",
    "    \"Count\": label_counts.values,\n",
    "    \"Percentage\": (label_counts.values / len(train_df) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(label_stats)\n",
    "\n",
    "# Add 'non_toxic' to label counts\n",
    "label_counts_with_clean = label_counts.copy()\n",
    "label_counts_with_clean['non_toxic'] = non_toxic_count\n",
    "label_counts_with_clean = label_counts_with_clean.sort_values(ascending=False)\n",
    "\n",
    "# Plot updated class distribution\n",
    "plt.figure(figsize=(10, 6), dpi=DPI)\n",
    "label_counts_with_clean.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Label Distribution Including Non-Toxic Comments\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ef8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=DPI)\n",
    "corr = train_df[label_cols].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Label Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence = train_df[label_cols].T.dot(train_df[label_cols])\n",
    "plt.figure(figsize=(8, 6), dpi=DPI)\n",
    "sns.heatmap(co_occurrence, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Label Co-occurrence Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3771aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6), dpi=DPI)\n",
    "train_df['comment_length'] = train_df['comment_text'].apply(lambda x: len(str(x).split()))\n",
    "train_df['comment_length'].hist(bins=50)\n",
    "plt.title(\"Comment Length Distribution\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cleaned'] = train_df['comment_text'].apply(clean_text)\n",
    "train_df['lemmatized'] = train_df['cleaned'].apply(lemmatizing)\n",
    "print(train_df[['comment_text', 'cleaned', 'lemmatized']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(10, 6), dpi=DPI)\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_df['is_toxic'] = train_df[label_cols].sum(axis=1) > 0\n",
    "\n",
    "# Plot histograms\n",
    "sns.histplot(data=train_df, x='word_count', hue='is_toxic', bins=50, palette={False: 'green', True: 'red'})\n",
    "\n",
    "# Plot settings\n",
    "plt.title('Word Count Distribution by Toxicity')\n",
    "plt.xlabel('Number of Words (Lemmatized)')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.legend(title='Toxic', labels=['Non-Toxic', 'Toxic'])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define toxicity labels\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Plot settings\n",
    "plt.figure(figsize=(20, 10), dpi=DPI)\n",
    "\n",
    "# Create one subplot per label (only for toxic comments)\n",
    "for i, label in enumerate(label_cols, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    filtered_df = train_df[train_df[label] == 1]\n",
    "    sns.histplot(data=filtered_df, x='word_count', bins=50, color='red', alpha=0.7)\n",
    "    plt.title(f'Word Count ({label}=1)')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Toxic Comment Count')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
