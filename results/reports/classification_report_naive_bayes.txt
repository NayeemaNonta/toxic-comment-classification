=== Baseline: naive_bayes ===

               precision    recall  f1-score   support

        toxic       0.92      0.51      0.66      3056
 severe_toxic       0.65      0.09      0.16       321
      obscene       0.91      0.50      0.65      1715
       threat       0.00      0.00      0.00        74
       insult       0.83      0.41      0.55      1614
identity_hate       0.69      0.04      0.07       294

    micro avg       0.89      0.44      0.59      7074
    macro avg       0.67      0.26      0.35      7074
 weighted avg       0.87      0.44      0.58      7074
  samples avg       0.05      0.04      0.04      7074


=== Metrics on 20% Test Set ===
Macro F1: 0.3483
Macro Precision: 0.6669
Macro Recall: 0.2593
Mean ROC AUC: 0.9457
