=== Baseline: logistic_liblinear ===

               precision    recall  f1-score   support

        toxic       0.90      0.61      0.73      3056
 severe_toxic       0.56      0.26      0.35       321
      obscene       0.92      0.63      0.74      1715
       threat       0.73      0.15      0.25        74
       insult       0.82      0.50      0.62      1614
identity_hate       0.73      0.15      0.25       294

    micro avg       0.88      0.55      0.67      7074
    macro avg       0.78      0.38      0.49      7074
 weighted avg       0.86      0.55      0.67      7074
  samples avg       0.06      0.05      0.05      7074


=== Metrics on 20% Test Set ===
Macro F1: 0.4913
Macro Precision: 0.7781
Macro Recall: 0.3824
Mean ROC AUC: 0.9754
