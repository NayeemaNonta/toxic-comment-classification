=== Baseline: logistic_liblinear ===

               precision    recall  f1-score   support

        toxic       0.91      0.59      0.72      3124
 severe_toxic       0.56      0.24      0.34       326
      obscene       0.93      0.64      0.76      1751
       threat       0.62      0.10      0.18        98
       insult       0.83      0.52      0.64      1613
identity_hate       0.65      0.18      0.28       295

    micro avg       0.88      0.55      0.67      7207
    macro avg       0.75      0.38      0.48      7207
 weighted avg       0.87      0.55      0.67      7207
  samples avg       0.05      0.05      0.05      7207


=== Metrics on 20% Test Set ===
Macro F1: 0.4828
Macro Precision: 0.7514
Macro Recall: 0.3767
Mean ROC AUC: 0.9761
